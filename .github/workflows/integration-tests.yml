name: Integration Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run integration tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  integration-tests:
    name: Integration Tests
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']
        exclude:
          # Reduce matrix size for faster CI
          - os: windows-latest
            python-version: '3.8'
          - os: macos-latest
            python-version: '3.8'
          - os: windows-latest
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.9'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install system dependencies (Linux)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libudev-dev \
          libv4l-dev \
          v4l-utils \
          pkg-config
        # Create mock video devices for testing
        sudo modprobe v4l2loopback devices=2 || true

    - name: Install system dependencies (macOS)
      if: runner.os == 'macOS'
      run: |
        # Install any macOS-specific dependencies if needed
        brew install pkg-config || true

    - name: Install system dependencies (Windows)
      if: runner.os == 'Windows'
      run: |
        # Windows dependencies are handled by pip packages
        echo "Windows dependencies handled by Python packages"

    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ matrix.python-version }}-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-${{ matrix.python-version }}-
          ${{ runner.os }}-pip-

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test,tui]"
        
        # Install platform-specific enhanced dependencies
        if [ "$RUNNER_OS" == "Linux" ]; then
          pip install -e ".[linux-enhanced]" || echo "Linux enhanced dependencies not available"
        elif [ "$RUNNER_OS" == "Windows" ]; then
          pip install -e ".[windows-enhanced]" || echo "Windows enhanced dependencies not available"
        elif [ "$RUNNER_OS" == "macOS" ]; then
          pip install -e ".[macos-enhanced]" || echo "macOS enhanced dependencies not available"
        fi
      shell: bash

    - name: Run unit tests
      run: |
        pytest tests/ -v \
          --tb=short \
          --cov=stablecam \
          --cov-report=xml \
          --cov-report=term-missing \
          -m "not slow and not integration"

    - name: Run integration tests
      run: |
        pytest tests/test_integration.py -v \
          --tb=short \
          -m "integration" \
          --maxfail=5

    - name: Run cross-platform compatibility tests
      run: |
        pytest tests/test_integration.py::TestCrossPlatformCompatibility -v \
          --tb=short \
          --maxfail=3

    - name: Run performance tests (quick)
      run: |
        pytest tests/test_performance.py -v \
          --tb=short \
          -m "not slow" \
          --maxfail=3

    - name: Upload coverage to Codecov
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.11'
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration
        name: codecov-integration

  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[perf]')

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libudev-dev \
          libv4l-dev \
          v4l-utils \
          pkg-config
        sudo modprobe v4l2loopback devices=5 || true

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test,tui,linux-enhanced]"
        pip install psutil  # For memory monitoring

    - name: Run performance tests
      run: |
        pytest tests/test_performance.py -v \
          --tb=short \
          -m "slow" \
          --maxfail=2 \
          --durations=10

    - name: Run stress tests
      run: |
        pytest tests/test_performance.py::TestStressPerformance -v \
          --tb=short \
          --maxfail=1

  tui-tests:
    name: TUI Integration Tests
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest]
        python-version: ['3.10', '3.11']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install system dependencies (Linux)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y libudev-dev libv4l-dev v4l-utils

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test,tui]"

    - name: Run TUI integration tests
      run: |
        pytest tests/test_integration.py::TestTUIIntegration -v \
          --tb=short \
          -m "integration" \
          --maxfail=3

    - name: Run TUI unit tests
      run: |
        pytest tests/test_tui.py -v \
          --tb=short \
          --maxfail=3

  end-to-end-tests:
    name: End-to-End Scenario Tests
    runs-on: ubuntu-latest
    needs: integration-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libudev-dev \
          libv4l-dev \
          v4l-utils \
          pkg-config
        # Create multiple mock video devices
        sudo modprobe v4l2loopback devices=10 || true

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test,tui,linux-enhanced]"

    - name: Run end-to-end scenario tests
      run: |
        pytest tests/test_integration.py::TestEndToEndScenarios -v \
          --tb=short \
          --maxfail=2

    - name: Run system integration tests
      run: |
        pytest tests/test_integration.py::TestSystemIntegration -v \
          --tb=short \
          --maxfail=2

    - name: Test CLI integration
      run: |
        # Test CLI commands work
        python -m stablecam --help
        echo "CLI help command successful"

  compatibility-matrix:
    name: Compatibility Matrix
    runs-on: ${{ matrix.os }}
    if: github.event_name == 'schedule'
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-20.04, ubuntu-22.04, windows-2019, windows-2022, macos-11, macos-12]
        python-version: ['3.8', '3.12']

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test]"

    - name: Run basic compatibility tests
      run: |
        pytest tests/test_integration.py::TestCrossPlatformCompatibility::test_platform_backend_selection -v
        pytest tests/test_integration.py::TestCrossPlatformCompatibility::test_hardware_id_generation_consistency -v

  security-tests:
    name: Security Integration Tests
    runs-on: ubuntu-latest
    needs: integration-tests

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install bandit safety

    - name: Run security tests
      run: |
        # Test concurrent access safety
        pytest tests/test_integration.py::TestEndToEndScenarios::test_concurrent_access_safety -v
        
        # Run security linting
        bandit -r stablecam/ -f json -o bandit-report.json || true
        
        # Check for known vulnerabilities
        safety check --json --output safety-report.json || true

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  benchmark-comparison:
    name: Performance Benchmark Comparison
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
    - name: Checkout PR code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev,test]"
        pip install psutil

    - name: Run performance benchmarks (PR)
      run: |
        pytest tests/test_performance.py::TestDetectionPerformance::test_detection_scaling_performance -v \
          --tb=short \
          -k "device_count-10" > pr-benchmark.txt || true

    - name: Checkout main branch
      uses: actions/checkout@v4
      with:
        ref: main
        path: main-branch

    - name: Run performance benchmarks (main)
      run: |
        cd main-branch
        python -m pip install -e ".[dev,test]"
        pytest tests/test_performance.py::TestDetectionPerformance::test_detection_scaling_performance -v \
          --tb=short \
          -k "device_count-10" > ../main-benchmark.txt || true

    - name: Compare benchmarks
      run: |
        echo "=== PR Benchmark ===" >> benchmark-comparison.txt
        cat pr-benchmark.txt >> benchmark-comparison.txt || true
        echo "=== Main Benchmark ===" >> benchmark-comparison.txt
        cat main-benchmark.txt >> benchmark-comparison.txt || true

    - name: Upload benchmark comparison
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-comparison
        path: benchmark-comparison.txt